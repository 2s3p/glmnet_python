{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Family\n",
    "\n",
    "`gaussian ` is the default family option in the function `glmnet`. Suppose we have observations $x_i \\in \\mathbb{R}^p$ and the responses $y_i \\in \\mathbb{R}, i = 1, \\ldots, N$. The objective function for the Gaussian family is\n",
    "$$\n",
    "\\min_{(\\beta_0, \\beta) \\in \\mathbb{R}^{p+1}}\\frac{1}{2N} \\sum_{i=1}^N (y_i -\\beta_0-x_i^T \\beta)^2+\\lambda \\left[ (1-\\alpha)||\\beta||_2^2/2 + \\alpha||\\beta||_1\\right],\n",
    "$$\n",
    "where $\\lambda \\geq 0$ is a complexity parameter and $0 \\leq \\alpha \\leq 1$ is a compromise between ridge ($\\alpha = 0$) and lasso ($\\alpha = 1$).\n",
    "\n",
    "Coordinate descent is applied to solve the problem. Specifically, suppose we have current estimates $\\tilde{\\beta_0}$ and $\\tilde{\\beta}_\\ell$ $\\forall j\\in 1,]\\ldots,p$. By computing the gradient at $\\beta_j = \\tilde{\\beta}_j$ and simple calculus, the update is\n",
    "$$\n",
    "\\tilde{\\beta}_j \\leftarrow \\frac{S(\\frac{1}{N}\\sum_{i=1}^N x_{ij}(y_i-\\tilde{y}_i^{(j)}),\\lambda \\alpha)}{1+\\lambda(1-\\alpha)},\n",
    "$$\n",
    "where $\\tilde{y}_i^{(j)} = \\tilde{\\beta}_0 + \\sum_{\\ell \\neq j} x_{i\\ell} \\tilde{\\beta}_\\ell$, and $S(z, \\gamma)$ is the soft-thresholding operator with value $\\text{sign}(z)(|z|-\\gamma)_+$.\n",
    "\n",
    "This formula above applies when the `x` variables are standardized to have unit variance (the default); it is slightly more complicated when they are not. Note that for \"family=gaussian\", `glmnet` standardizes $y$ to have unit variance before computing its lambda sequence (and then unstandardizes the resulting coefficients); if you wish to reproduce/compare results with other software, best to supply a standardized $y$ first (Using the \"1/N\" variance formula).\n",
    "\n",
    "`glmnet` provides various options for users to customize the fit. We introduce some commonly used options here and they can be specified in the `glmnet` function.\n",
    "\n",
    "* `alpha` is for the elastic-net mixing parameter $\\alpha$, with range $\\alpha \\in [0,1]$. $\\alpha = 1$ is the lasso (default) and $\\alpha = 0$ is the ridge.\n",
    "\n",
    "* `weights` is for the observation weights. Default is 1 for each observation. (Note: `glmnet` rescales the weights to sum to N, the sample size.)\n",
    "\n",
    "* `nlambda` is the number of $\\lambda$ values in the sequence. Default is 100.\n",
    "\n",
    "* `lambda` can be provided, but is typically not and the program constructs a sequence. When automatically generated, the $\\lambda$ sequence is determined by `lambda.max` and `lambda.min.ratio`. The latter is the ratio of smallest value of the generated  $\\lambda$ sequence (say `lambda.min`) to `lambda.max`.  The program then generated `nlambda` values linear on the log scale from `lambda.max` down to `lambda.min`. `lambda.max` is not given, but easily computed from the input $x$ and $y$; it is the smallest value for `lambda` such that all the coefficients are zero.  For `alpha=0` (ridge) `lambda.max` would be $\\infty$; hence for this case we pick a value corresponding to a small value for `alpha` close to zero.)\n",
    "\n",
    "* `standardize` is a logical flag for `x` variable standardization, prior to fitting the model sequence. The coefficients are always returned on the original scale. Default is `standardize=TRUE`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
